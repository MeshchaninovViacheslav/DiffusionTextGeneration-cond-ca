{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09baff73-7388-4cf3-be75-1448d515e7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d130a75b-6f55-48ed-821e-afa12cd52215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f108e8ea-04b0-4487-aad0-5b87b0fdd1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        return self.layer(x1 + x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6867a7c2-acbb-4d08-b1a6-a895f7923b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7605931-56b0-4787-a695-8363e414a67b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0 = torch.randn(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x1 = model(x0, x0)\n",
    "    x1 = x1.detach()\n",
    "    \n",
    "x2 = model(x1, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e3fcbb-af64-44c3-881b-d4f067831a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = torch.square(x2 - x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3e76450-4e16-42a7-b17a-b3f49f203ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6fc0ee1-b075-4bd7-b0e9-6e77f779be6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.weight False\n",
      "layer.bias False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "            print(name, param.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63711bcf-9932-4d85-af52-a1024dab624f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c65d1f0-3842-4791-add9-22723dc38c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, disable_progress_bar\n",
    "from datasets.utils.logging import set_verbosity_error\n",
    "from itertools import cycle\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9abcc2-9616-4657-9e57-ab9b045664b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RocStoryDatasetDDP:\n",
    "    def __init__(self,\n",
    "                 split, tokenizer_bert, tokenizer_cond, tokenizer_gen, max_sequence_len,\n",
    "                 pos_begin: float = 0., pos_end: float = 0.67):\n",
    "        self.split = split\n",
    "        self.tokenizer_bert = tokenizer_bert\n",
    "        self.tokenizer_cond = tokenizer_cond\n",
    "        self.tokenizer_gen = tokenizer_gen\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.max_cond_len = max_sequence_len\n",
    "        self.pos_begin = pos_begin\n",
    "        self.pos_end = pos_end\n",
    "        self.device_id = dist.get_rank() if torch.distributed.is_initialized() else 0\n",
    "        self.total_device_number = dist.get_world_size() if torch.distributed.is_initialized() else 1\n",
    "        self.epoch = 0\n",
    "\n",
    "    def spilt_data_across_gpu(self, dt: List[str]):\n",
    "        if self.split == \"train\":\n",
    "            indexes = np.random.default_rng(seed=self.epoch).permutation(len(dt))\n",
    "        else:\n",
    "            indexes = np.arange(len(dt))\n",
    "        \n",
    "        start_ind = self.device_id * (len(dt) // self.total_device_number)\n",
    "        end_ind = (self.device_id + 1) * (len(dt) // self.total_device_number)\n",
    "        if (self.device_id + 1) == self.total_device_number:\n",
    "            indexes = indexes[start_ind:]\n",
    "        else:\n",
    "            indexes = indexes[start_ind: end_ind]\n",
    "        \n",
    "        dt = [dt[i] for i in indexes]\n",
    "        return dt\n",
    "    \n",
    "\n",
    "    def load_data(self, path):\n",
    "        dt = []\n",
    "        with open(path, \"r\") as file:\n",
    "            for l in file:\n",
    "                dt.append(l.strip())\n",
    "        dt = self.spilt_data_across_gpu(dt)\n",
    "        dt = Dataset.from_list([{\"text\": t} for t in dt])\n",
    "\n",
    "        self.dt = dt.map(\n",
    "            self.batch_preprocessing,\n",
    "            batched=True,\n",
    "            load_from_cache_file=False,\n",
    "            num_proc=30,\n",
    "            desc=\"Dataset tokenization\",\n",
    "            batch_size=1000,\n",
    "        )\n",
    "        self.dt = self.dt.with_format(\"pt\", columns=[\"input_ids\", \"cond_ids\", \"input_mask\", \"cond_mask\"])\n",
    "        return self.dt\n",
    "\n",
    "    def batch_preprocessing(self, batch):\n",
    "        # Tokenize\n",
    "        input_ids = self.tokenizer_cond(\n",
    "            batch[\"text\"],\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_sequence_len,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # Random split\n",
    "        batch_size = len(batch[\"text\"])\n",
    "        elem_counts = self.max_cond_len\n",
    "        delimeter_poses = (\n",
    "            (\n",
    "                    np.random.rand(batch_size) *\n",
    "                    (self.pos_end - self.pos_begin) + self.pos_begin\n",
    "            ) * elem_counts\n",
    "        ).astype(int)\n",
    "\n",
    "        cond_ids_list = []\n",
    "        input_ids_list = []\n",
    "        for i, element_ids in enumerate(input_ids):\n",
    "            cond_ids_list.append(element_ids[:delimeter_poses[i]])\n",
    "            input_ids_list.append(element_ids[delimeter_poses[i]:])\n",
    "        \n",
    "\n",
    "        # Tokens decode\n",
    "        texts_cond = self.tokenizer_bert.batch_decode(cond_ids_list, skip_special_tokens=True)\n",
    "        texts_input = self.tokenizer_bert.batch_decode(input_ids_list, skip_special_tokens=True)\n",
    "\n",
    "        # Text encode\n",
    "        cond_ = self.tokenizer_cond(\n",
    "            texts_cond,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_sequence_len,\n",
    "        )\n",
    "\n",
    "        input_ = self.tokenizer_gen(\n",
    "            texts_input,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_sequence_len,\n",
    "        )\n",
    "\n",
    "        output = {\n",
    "            \"input_ids\": input_[\"input_ids\"],\n",
    "            \"cond_ids\": cond_[\"input_ids\"],\n",
    "            \"input_mask\": input_[\"attention_mask\"],\n",
    "            \"cond_mask\": cond_[\"attention_mask\"],\n",
    "        }\n",
    "        return output\n",
    "\n",
    "\n",
    "    def clear_data(self):\n",
    "        del self.dt\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def get_data(self):\n",
    "        if self.split == \"valid\":\n",
    "            while True:\n",
    "                test_path = \"/home/vmeshchaninov/nlp_models/data/rocstories/validation/data.txt\"\n",
    "                yield self.load_data(test_path)\n",
    "        elif self.split == \"train\":\n",
    "            while True:\n",
    "                train_path = \"/home/vmeshchaninov/nlp_models/data/rocstories/train/data.txt\"\n",
    "                yield self.load_data(train_path)\n",
    "        else:\n",
    "            raise Exception(\"Wrong data split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b228d53-cea4-4795-8099-ecbd8e0e4b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85f2b7d-b3b3-4dd6-91f8-59be96250866",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_cfg = \"bert-base-uncased\"\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(bert_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9741d5-fb97-4654-82a5-adff723c6de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt = RocStoryDatasetDDP(\"valid\", tokenizer_bert, tokenizer_bert, tokenizer_bert, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6db5155-6c90-44c3-a456-63d9d629de6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64ec6aaf92946619f9475e607c2c642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset tokenization (num_proc=30):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = next(dt.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dcffa01-b9d1-48e0-8aaf-d0b9c203a854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] susy was a little bit apprehensive. so [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.decode(dt[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76cec0-0456-4c11-8ad0-170c56b6439f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-fap2_env]",
   "language": "python",
   "name": "conda-env-.conda-fap2_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
