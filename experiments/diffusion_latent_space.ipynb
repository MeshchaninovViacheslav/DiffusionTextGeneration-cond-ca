{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f55db5a-4896-4cb9-9088-bc45880d4f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef319fcd-c8a9-483e-a363-b0a56545f98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44de78fa-0c46-4bab-9e4c-dd8ba614191f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusion_holder import DiffusionRunner\n",
    "from transformers import BertConfig, BertTokenizerFast\n",
    "\n",
    "from diffusion_holder import DiffusionRunner\n",
    "from utils.util import set_seed, dict_to_cuda\n",
    "from estimation_utils.util import estimate_model, reduce_metrics, gather_texts\n",
    "import diffusion_utils.schedulers as schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e3bdea-fbef-49f5-81ef-ed1c36aeacca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4a11a7-5380-463d-a6c3-dd604b23a544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_config():\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    training = config.training = ml_collections.ConfigDict()\n",
    "    training.training_iters = 500_000\n",
    "    training.training_iters = training.training_iters\n",
    "    training.checkpoint_freq = 50_000\n",
    "    training.eval_freq = 5_000\n",
    "    training.batch_size = 512  # * 8\n",
    "\n",
    "    training.ode_sampling = False\n",
    "    training.checkpoints_folder = '../checkpoints/'\n",
    "    config.checkpoints_prefix = ''\n",
    "\n",
    "    loss = config.loss = ml_collections.ConfigDict()\n",
    "    loss.ce_coef = 0.\n",
    "\n",
    "    refresh = config.refresh = ml_collections.ConfigDict()\n",
    "    refresh.true = False\n",
    "    refresh.prefix = \"./checkpoints/wikipedia--t5-bert-self_cond_500000_.pth\"\n",
    "    refresh.wand_id = \"g5fb4af3\"\n",
    "\n",
    "    validation = config.validation = ml_collections.ConfigDict()\n",
    "    validation.batch_size = 4\n",
    "    validation.validation_iters = int(10_000 / validation.batch_size)\n",
    "    validation.num_gen_texts = 8192\n",
    "    validation.p_uncond = 0.\n",
    "\n",
    "    dynamic = config.dynamic = ml_collections.ConfigDict()\n",
    "    dynamic.solver = 'euler'\n",
    "    dynamic.scheduler = \"sd\"\n",
    "    dynamic.N = 200\n",
    "    dynamic.beta_min = 0.1\n",
    "    dynamic.beta_max = 20\n",
    "    dynamic.ode_sampling = False\n",
    "    dynamic.coef_d = 10\n",
    "\n",
    "    model = config.model = ml_collections.ConfigDict()\n",
    "    model.ema_rate = 0.9999\n",
    "    model.enc_type = \"base\"\n",
    "    model.embeddings_type = \"embeddings\"\n",
    "    model.dif_enc_type = \"base\"\n",
    "    model.downstream_task = \"\"  # \"qqp\"\n",
    "    model.dataset = \"wikipedia\"  # \"glue\"\n",
    "    model.prediction = \"x_0\"\n",
    "    model.loss = \"L_x_0\"\n",
    "    model.decoder_path = \"decoder-wikipedia-128.pth\"\n",
    "\n",
    "    data = config.data = ml_collections.ConfigDict()\n",
    "    data.max_sequence_len = 64\n",
    "    data.pos_begin = 0.0\n",
    "    data.pos_end = 0.67\n",
    "    data.enc_bert_mean = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-bert_base-wiki-mean.pt\"\n",
    "    data.enc_bert_std = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-bert_base-wiki-std.pt\"\n",
    "\n",
    "    data.enc_t5_mean = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-t5-wiki-mean.pth\"\n",
    "    data.enc_t5_std = \"/home/vmeshchaninov/DiffusionTextGeneration-cond-ca/data/encodings-t5-wiki-std.pth\"\n",
    "\n",
    "    config.finetuning = False\n",
    "    config.seed = 0\n",
    "    config.ddp = False\n",
    "    config.bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "    config.use_self_cond = False\n",
    "    config.project_name = \"test\" #\"dtg-exps-1.0\"\n",
    "    config.timesteps = \"linear\"\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2216bf3b-a34a-4a20-90cd-2469a9cc401e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmeshchaninov/.conda/envs/fap2_env/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5EncoderModel: ['decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.final_layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['enc_normalizer.enc_mean', 'enc_normalizer.enc_std']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoderModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertEncoderModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['enc_normalizer.enc_mean', 'enc_normalizer.enc_std']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206fd687a1994269805a1a156756bb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset tokenization (num_proc=30):   0%|          | 0/38661 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = create_config()\n",
    "config.checkpoints_prefix = \"wikipedia--t5-bert-initial_last_\"\n",
    "\n",
    "diffusion = DiffusionRunner(config, latent_mode=config.model.embeddings_type, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d6ef56-f084-483d-b117-6000602cbfeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmeshchaninov/.conda/envs/fap2_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "diffusion.set_valid_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5289794e-a461-47af-8fa5-61b22a613194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = iter(diffusion.valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94551c67-cd3c-4954-a2f6-3b4452951e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = next(loader)\n",
    "X = dict_to_cuda(X)\n",
    "\n",
    "with torch.no_grad():\n",
    "    clean_X = diffusion.encoder_gen(**{\"input_ids\": X[\"input_ids\"], \"attention_mask\": X[\"input_mask\"]})\n",
    "    cond_X = diffusion.encoder_cond(**{\"input_ids\": X[\"cond_ids\"], \"attention_mask\": X[\"cond_mask\"]})\n",
    "    \n",
    "cond_mask = X[\"cond_mask\"]\n",
    "attention_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bf8ae1-01a1-4075-8879-ff2965f3c761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps_t = 1. / diffusion.dynamic.N\n",
    "timesteps = torch.linspace(diffusion.dynamic.T, eps_t, diffusion.dynamic.N, device=diffusion.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bebcd4dd-541a-4843-97cb-5e7eb12a18ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_list = []\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50e7b8a0-2c39-4780-b6f3-35bb43ea96a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 41.33it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = config.validation.batch_size\n",
    "    x = diffusion.dynamic.prior_sampling(clean_X.shape).to(diffusion.device)\n",
    "\n",
    "    for idx in tqdm(range(diffusion.dynamic.N)):\n",
    "        t = timesteps[idx]\n",
    "        next_t = timesteps[idx + 1] if idx < diffusion.dynamic.N - 1 else eps_t\n",
    "\n",
    "        input_t = t * torch.ones(batch_size, device=diffusion.device)\n",
    "        next_input_t = next_t * torch.ones(batch_size, device=diffusion.device)\n",
    "\n",
    "        output = diffusion.diff_eq_solver.step(\n",
    "            x_t=x, t=input_t, next_t=next_input_t,\n",
    "            cond=cond_X,\n",
    "            cond_mask=cond_mask,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        x, x_mean = output[\"x\"], output[\"x_mean\"]\n",
    "        x_list.append(output[\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dc01899-20f8-46ef-8207-16945d972770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def make_gif_from_list_of_tensors(x_list):\n",
    "    images = []\n",
    "    for idx in range(diffusion.dynamic.N):\n",
    "        embeds = x_list[idx][0].cpu().numpy()\n",
    "        image = np.array(embeds[:, :128])\n",
    "        image -= image.min()\n",
    "        image /= image.max()\n",
    "\n",
    "        image = Image.fromarray(np.uint8(image * 255))\n",
    "        image = image.resize((128 * 4, 32 * 4), resample=Image.NEAREST)\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "    images[0].save(\n",
    "                'animation-x_t.gif',\n",
    "                save_all=True,\n",
    "                append_images=images[1:], # append rest of the images\n",
    "                duration=100, # in milliseconds\n",
    "                loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75a721de-daed-4b3d-9ca7-0388b97df27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_gif_from_list_of_tensors(x_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e47d8-62e8-4780-b59e-28478cffa8e1",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2b7d11f-469c-4d6e-8c8b-6998134cc38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_list = []\n",
    "token_list = []\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9697f844-7be1-46b7-9362-941ca2179510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:05<00:00, 39.32it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = config.validation.batch_size\n",
    "    x = diffusion.dynamic.prior_sampling(clean_X.shape).to(diffusion.device)\n",
    "\n",
    "    for idx in tqdm(range(diffusion.dynamic.N)):\n",
    "        t = timesteps[idx]\n",
    "        next_t = timesteps[idx + 1] if idx < diffusion.dynamic.N - 1 else eps_t\n",
    "\n",
    "        input_t = t * torch.ones(batch_size, device=diffusion.device)\n",
    "        next_input_t = next_t * torch.ones(batch_size, device=diffusion.device)\n",
    "\n",
    "        output = diffusion.diff_eq_solver.step(\n",
    "            x_t=x, t=input_t, next_t=next_input_t,\n",
    "            cond=cond_X,\n",
    "            cond_mask=cond_mask,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        x, x_mean = output[\"x\"], output[\"x_mean\"]\n",
    "        \n",
    "        tokens = diffusion.pred_logits(output[\"x_0\"]).argmax(dim=-1)[:, :16]\n",
    "        token_list.append(tokens[0].cpu().numpy())\n",
    "        text = diffusion.tokenizer_gen.batch_decode(tokens, skip_special_tokens=True)[0]\n",
    "        text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a85bd5ce-a266-4eb0-ae07-0238e366fb20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -          and        and        and        and        and        and        and        -          and        and        -          and        and        -          \n",
      "1  -          and        and        and        and        and        and        and        and        and        and        and        and        and        -          \n",
      "2  -          and        and        and        and        and        and        and        and        and        -          -          and        and        and        \n",
      "3  -          and        and        -          and        and        and        and        and        and        -          -          and        and        and        \n",
      "4  '          and        and        .          and        and        and        and        and        and        the        the        and        and        and        \n",
      "5  and        and        and        -          -          and        and        and        and        and        and        and        and        and        and        \n",
      "6  and        and        to         the        and        trim       -          and        and        and        and        and        and        and        and        \n",
      "7  -          and        to         a          -          trimtop    and        and        as         and        the        and        -top       \n",
      "8  -          -          of         a          gen        sedan      sedan      and        was        and        and        to         and        -          seat       \n",
      "9  base       lineup     with       a          4          -          5          -          model      and        and        to         a          -          -          \n",
      "10 base       model      with       a          4          -          5          -          axle       sedan      ,          it         was        is         -          \n",
      "11 base       model      with       a          four       -          of         -          seat       windshield ,          and        it         is         to         \n",
      "12 base       model      with       a          4          -          5          -          charge     windshield ,          so         it         is         not        \n",
      "13 base       lineup     with       a          4          -          2          rear       engine     engines    ,          so         it         is         not        \n",
      "14 base       1997       with       a          gen        -          2          -          gen        engines    ,          so         it         is         not        \n",
      "15 compact    lineup     with       a          4          -          hand       gen        gen        models     ,          so         it         is         really     \n",
      "16 model      model      with       a          4          -          seat       rear       gen        engines    ,          and        it         is         actually   \n",
      "17 now        equipped   with       a          4          -          backseat   gen        cluster    backseat   ,          and        it         is         just       \n",
      "18 also       equipped   with       a          4          -          seat       gen        cluster    windshield ,          and        it         is         actually   \n",
      "19 mostly     equipped   with       a          4          -          backseat   rearwd     engine     ,          and        it         is         obviously  \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate([t.replace(\",\", \" ,\").replace(\".\", \" .\").split() for t in text_list[::10]]):\n",
    "    n = 11\n",
    "    s = f\"{i}{' ' * (2 - len(str(i)))} \"\n",
    "    for c in t:\n",
    "        s += f\"{c}{' ' * (n - len(str(c)))}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "401c053b-4a3d-456c-8688-61a650f42224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  101   1011  1998  1998  1998  1998  1998  1998  1998  1011  1998  1998  1011  1998  1998  1011  \n",
      "1  101   1011  1998  1998  1998  1998  1998  1998  1998  1998  1998  1998  1998  1998  1998  1011  \n",
      "2  101   1011  1998  1998  1998  1998  1998  1998  1998  1998  1998  1011  1011  1998  1998  1998  \n",
      "3  101   1011  1998  1998  1011  1998  1998  1998  1998  1998  1998  1011  1011  1998  1998  1998  \n",
      "4  101   1005  1998  1998  1012  1998  1998  1998  1998  1998  1998  1996  1996  1998  1998  1998  \n",
      "5  101   1998  1998  1998  1011  1011  1998  1998  1998  1998  1998  1998  1998  1998  1998  1998  \n",
      "6  101   1998  1998  2000  1996  1998  12241 1011  1998  1998  1998  1998  1998  1998  1998  1998  \n",
      "7  101   1011  1998  2000  1037  1011  12241 14399 1998  1998  2004  1998  1996  1998  1011  14399 \n",
      "8  101   1011  1011  1997  1037  8991  15134 15134 1998  2001  1998  1998  2000  1998  1011  2835  \n",
      "9  101   2918  10515 2007  1037  1018  1011  1019  1011  2944  1998  1998  2000  1037  1011  1011  \n",
      "10 101   2918  2944  2007  1037  1018  1011  1019  1011  17290 15134 1010  2009  2001  2003  1011  \n",
      "11 101   2918  2944  2007  1037  2176  1011  1997  1011  2835  19521 1010  1998  2009  2003  2000  \n",
      "12 101   2918  2944  2007  1037  1018  1011  1019  1011  3715  19521 1010  2061  2009  2003  2025  \n",
      "13 101   2918  10515 2007  1037  1018  1011  1016  4373  3194  5209  1010  2061  2009  2003  2025  \n",
      "14 101   2918  2722  2007  1037  8991  1011  1016  1011  8991  5209  1010  2061  2009  2003  2025  \n",
      "15 101   9233  10515 2007  1037  1018  1011  2192  8991  8991  4275  1010  2061  2009  2003  2428  \n",
      "16 101   2944  2944  2007  1037  1018  1011  2835  4373  8991  5209  1010  1998  2009  2003  2941  \n",
      "17 101   2085  6055  2007  1037  1018  1011  19978 8991  9324  19978 1010  1998  2009  2003  2074  \n",
      "18 101   2036  6055  2007  1037  1018  1011  2835  8991  9324  19521 1010  1998  2009  2003  2941  \n",
      "19 101   3262  6055  2007  1037  1018  1011  19978 4373  21724 3194  1010  1998  2009  2003  5525  \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(token_list[::10]):\n",
    "    n = 6\n",
    "    s = f\"{i}{' ' * (2 - len(str(i)))} \"\n",
    "    for c in t:\n",
    "        s += f\"{c}{' ' * (n - len(str(c)))}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34e011-a406-401d-8ec6-0ca095ff70be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-fap2_env]",
   "language": "python",
   "name": "conda-env-.conda-fap2_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
